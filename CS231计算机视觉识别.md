## CS231计算机视觉识别

[TOC]

### 2 图像分类

#### 2.1 数据驱动方法

CNN提取特征 识别图片分类

l1距离，曼哈顿距离，比较图片

最邻近分类器

训练器保存训练数据

对每一个测试图片，计算输入图像与训练样本图像的L1距离，找出最小距离的图像，即预测的分类标签。 

k临近分类器

不是直接用最近的样例的类别作为预测的分类标签，而是K个最邻近的点中的权重较高的结果

#### 2.2 K最邻近算法

l2距离，欧式距离 不能很好的衡量图片的区别

设置超参数的方法：将数据集分为训练集，验证集（用于效果比较和调餐），测试集（确定参数后用测试集跑一个模型性能效果出来）

#### 2.3 线性分类

神经网络的组成单元

将图像拉成一个矩阵，通过数据训练出一个参数矩阵W

线性分类是每个种类的学习模版，对于图像中的每一个像素，和十个类中的每一个类，矩阵W中都有一些对应的项告诉我们那个像素对那个分类有多少影响，也就是说矩阵W里的每一行都对应一个类的模版。

如果我们还原这些行的值成图片的大小，那么每一行又分别对应一些权重——虚像像素值和对应的那个类别，然后我们就可以可视化的学到每个类的模版。

线性分类器也可定义为学习像素在高维空间的一个线性决策边界，其中高维空间就对应了图片能取到的像素密度值

### 3 损失函数和优化

#### 3.1 损失函数

对于参数矩阵W的某个取值，我们可以用这个W获得任意图片在10个分类的得分，分数越高表示越可能属于这个类。

W有多种取值可能，需要度量任意某个W的好坏，这个度量函数就是损失函数。

通用损失函数公式$L=\frac{1}{N}\Sigma L_i (f(x_i , W), y_i)$

**多类别SVM的损失函数**

对于每个测试样例，对每个错误的类别求损失，如果正确分类的分数$s_{yi}$比错误分类的分数$s_j$大过某个安全边距，那么损失是0。最终损失是对每个错误分类的损失求和。

$L_i = \sum\limits_{j\neq y_i} \begin{cases} 0，if s_{yi} \geq s_j + 1 \\ s_j-s_{yi}+1， otherwise\end{cases}    $

也就是

$L_i=\sum\limits_{j\neq y_i}max(0,s_j-s_{yi}+1)$

最终对整个数据集求损失$L=\frac{1}{N}\sum_{i=1}^{N}L_i$

调试策略：

当第一次训练时，基本一张图在每个类别的分数都相同，那最终损失函数=类别数-1（正确的当前类别）

合页损失函数，当只有一点错误时不在意，多个错误就放大它的效果

平方损失函数，放大微小的错误

softmax分类器多项逻辑回归，对原本的在每个分类上的分数先指数后正则化表示出概率分布P（此时Pi之和为1），损失函数为$L_i=-logP(Y=y_i|X=x_i)$,目标是 最大化正确类别的logP

对损失函数正则化防止过拟合，降低幂数，降低复杂度

#### 3.2 优化

梯度，偏导数组成的向量，负梯度方向指向函数下降最快的方向

步长，学习率，最重要的超参数，最先确定好

使用梯度下降法使损失函数趋向最小，来确定W

计算梯度的方法：有限差分估计 解析梯度估计

*数值梯度用于单元测试确保推导正确

当数据集太大时，使用整个数据集，要很久时间才能更新一次W

因此使用随机梯度下降方法，每次选取minibatch小批量训练样例来估算误差综合以及实际梯度

提前抽取并保存图像特征，用这些特征训练模型，将图像特征转换可能更好取得分类边界：比如转换为极坐标，比如色谱图，方向梯度直方图HoG，词袋：从图像中抽取随机的图像块，聚簇后组成codebook，统计每种图像块出现频次

卷积神经网络并非提前记录特征，直接从数据中学习特征，将获取的像素值直接喂给CNN，经过多层计算最终得到一些数据驱动的特征表示类型。然后我们训练整个网络的全部权重（参数）

### 4 神经网络

#### 4.1 反向传播

计算任意复杂函数的解析梯度——计算图——用这张图表示函数，图的节点表示每一步计算->反向传播算法->递归地调用链式规则来计算图中每个变量的梯度：每个节点的梯度等于右边的上游节点传回来的梯度*本地梯度，而与其他节点无关

![image-20200131103239245](/Users/jiangtianmeng/Library/Application Support/typora-user-images/image-20200131103239245.png)

#### 4.2 神经网络

简单线性函数分层组合形成的复杂非线性函数，非常多层就是深度神经网络

将神经元节点组成全连接层的层，抽象成层便于使用向量化代码高效的计算（比如矩阵相乘，而不是一个个节点计算）

### 5 CNN

#### 5.1 CNN的历史 

1957第一代感知机 1960多层感知机网络 1986反向传播算法

2006深度神经网络可以训练，经过预训练来初始化网络参数，然后反向传播训练调优

2012CNNin语音识别 图像分类 AlexNet

图像边缘识别打标 自动驾驶 星系 路标

#### 5.2 卷积和池化

卷积总是可以写成矩阵相乘

**全连接层**

将一个32 * 32 * 3的图像拉伸成3072 * 1, 输入3072维矩阵和参数矩阵10 * 3072进行点积运算，得到一个激活值，作为这一神经元的值——一个数字：一行W和输入的3072维矩阵的点积结果 。这一层将有10个神经元输出

**卷积层**

卷积层和全连接层的区别，保全空间结构，32 * 32 * 3。W是一些小的卷积核，比如5 * 5 * 3，我们使用卷积核在整个图像上滑动，计算出每一个空间定位时的点积结果。滑动完后得到一张28 * 28 * 1的激活映射activation map

滑动方式，每个像素，每两个像素——步长

使用多种卷积核，得到不同的概念。每种卷积核得到一层激活映射，可以简单拼接形成一个新图像28 * 28 * 6（卷积核种类数）——卷积层

网络具有多个卷积层，一层层更提取到更复杂的特征

卷积神经网络的设计：常见的卷积核尺寸大小1|3|5，滑动步长1|2，卷积核个数，卷积核类型，输出图像尺寸，填充方式，池化的次数（交叉验证实验确定）

输出的activation map大小计算公式$(N-F)/stride+1$，N是图像原始宽度，F是卷积核宽度，stride滑动步长   

为了让输出图像大小完整必须是整数，stride只有有限个可行解

获取原尺寸输出的办法（因为一层层size缩小下来会丢失很多信息 ）：将原始图片周围添加一层值为0|复制边缘像素的padding，N变成9，(9-3)/1+1=7和原始图片相同大小。 

卷积神经网络就是很多个层的组合：卷积层，非线形层（比如Relu层），池化层，输出到全连接层，连接所有卷积层的输出，最终得到一个结果。  

使用caffe等框架直接在配置文件中设定卷积核大小，卷积核个数，步长，卷积核类型（高斯）

#### 5.3 视觉之外的卷积神经网络 

池化层使得表达更小更好管理，对每个activation map单独操作

经过池化（降采样处理）的图像深度不变，但长宽减半

池化层没有参数

常见方法max pooling：使用2 * 2	的卷积核步长为2，滑动时没有重叠，输出的图像尺寸即减半 好处：直观

需要设定的超参数：卷积核尺寸/池化的空间范围，滑动步长

常见设定2 * 2 | 3 * 3，步长2

最终的全连接层，不需要保持原有的结构了，将每一层的信息整合，拉平成一个一维向量来得到结论

趋势是：卷积核越来越小，池化层越来越少，卷积层越来越多，卷积神经神经网络越来越深

### 6训练CNN上

1.setup 激活函数，预处理，权重初始化，正则化，梯度检查

2.训练中的动态变化监控学习过程，更新参数，超参数优化

3.模型评估和模型集成

#### 6.1 setup

#### 激活函数

##### sigmoid函数 少用

1.如果输入的正数太大或负数太小，饱和的神经元（平了）会造成梯度消失的问题，梯度更新效率极低。 

2.sigmoid输出值不是以0为中心的

3.指数计算代价高（卷积点乘代价更高）

##### ReLU rectified linear unit 基本的

$f(x)=max(0,x)$

简单的max计算，收敛速度高

在输入值为正时，不会饱和

问题：

1.不是0中心

2.在负半轴都是0，会梯度消失，再也不会更新，dead relu

*偏置项初始化为0

##### leaky relu 可以试试

$f(x)=max(0.01x,x)$

在负半轴不会死掉

##### PReLu

$f(x)=max(\alpha x,x)$

$\alpha$作为一个可以反向传播学习的参数，具有更高的灵活性

##### ELU 可以试试

拥有均值接近0的输出，但负半轴没有倾斜，可以用于建立一个负饱和机制，使得模型对噪音更有鲁棒性，得到更健壮的反激活状态

$f(x)=\begin{cases}x,x > 0\\\alpha (exp(x)-1),x\leq0\end{cases}$

##### Maxout"Neuron"

集合了ReLU和leaky ReLU 不会死亡 但参数量翻倍

#### 预处理数据

归一化数据，到相同值域，但图像中不太需要

图像的预处理以0均值化就好——减去均值图像的值（均值图像是[32,32,3]的数组：尺寸32*32，3个颜色层次/深度）比如AlexNet|减去单通道图像均值（对均值图像取3个通道的均值，即平均化RGB）比如VGGNet

测试数据中同样处理

#### 初始化权重

法一：小的随机数，从高斯分布中采样*0.01。存在问题：因为太小了，每个神经单元做的事情类似，造成正向传播中所有的数据越来越小，反向传播中梯度和上层梯度都趋向于0

法二：增大权重，从高斯分布中采样*1。存在问题：如果用tan做激活函数，网络会饱和

好的经验使用Xavier初始化

从高斯分布中采样，然后根据我们拥有的输入的数量来进行缩放，要求输入的方差等于输出的方差。但用relu时有点问题，因为实际只有一半输入有活性，所以要/2

#### 批量归一化Batch Normalization 

目的：将数据转化为单位高斯数据，单位高斯函数激活，在每一层都有好的高斯分布，通过归一化（每一个神经元的均值和方差）

位置：BN层通常插入在全连接层或卷积层后，在nonlinearity非线性函数之前

做法：先归一化，再用常量$\gamma$缩放和$\beta$偏移

意义：恢复恒等函数，拥有更好的鲁棒性，使得下一层的非线性函数比如tan拥有更好的不饱和度，使得梯度流动更好，allow更高的学习率，减少对于初始化的依赖

#### 6.2 监控学习过程

先从小数据集开始，尽量拟合，关闭正则项检验损失可以降为0

正式训练，加上正则项，设置学习率（最重要）

交叉验证，用几个epoch看哪几个超参数有效。如果某个epoch中损失变为初始3倍以上，可以直接结束。

random search对超参数空间覆盖的更好 vs grid search 

![炼丹](/Users/jiangtianmeng/Desktop/pics/cnn/炼丹.png)

观察损失变化，来调整学习率

如果一开始loss不动是初始化不好

**我们的目标是降低损失函数+减少训练误差和测试误差的差距**

在训练集上的准确性越来越高但验证集停滞：过拟合：正则化

训练集上的准确性提升缓慢：欠拟合：增加项

范数

### 7 训练CNN下

#### 7.1 更好的优化 

##### SGD随机梯度下降法：问题鞍点、局部最小值

随机梯度下降+动量：保持一个不随时间变化的速度，并将梯度估计添加到这个速度上（向量运算），然后在这个速度的方向上step前进，而不是在梯度方向step前进。（想象小球滚下山，小球在鞍点梯度很小时却仍然有速度，按速度方向下降）

![SGD公式](/Users/jiangtianmeng/Desktop/pics/cnn/SGD公式.png)

![SGD+动量](/Users/jiangtianmeng/Desktop/pics/cnn/SGD+动量.png)

*还可尝试nesterov公式+换元法

##### AdaGrad算法 凸函数常用 CNN不太常用

在优化的过程中保持对每一步的梯度的平方和的持续估计

##### RMSProp 

AdaGrad的变体，但平方梯度*衰减率，使平方梯度下降

##### Adam 很好用的首选！ RMSProp + 动量 = 动量+第二个梯度平方

##### 学习率

学习率可以变化，持续衰减，用步长衰减的学习率

但是一开始先不衰减，观察损失函数曲线再设定该衰减的位置

#####  ？牛顿法：二阶偏导矩阵 海森矩阵 二次逼近 DP中不常用

##### L-BFGS二阶优化器 用于风格迁移 神经网络不常用

##### 集成技术 保留多个模型快照 然后集合

##### trick对不同时刻的某个参数求指数衰减平均值

####  7.2 正则化

batch normalization：提高单一模型的效果，防止过拟合

dropout：用在全连接层，随机将一些神经单元置为0，在测试时没有任何随机性，而是用dropout的概率乘以输出

数据增强：随机地转化图像，翻转平移等操作

随机池化：不常用

#### 7.3 迁移学习

不需要超大数据集也可以训练CNN

重新随机初始化最后的矩阵冻结前面的层

微调：在原始的整个数据集上充分训练之后最后一层收敛（泛化能力已经很强），可以更新整个网络的权值，此时将学习率调低，适应我的数据集。

下载别人预训练的模型，然后用自己的小数据集微调这个模型

### 8 深度学习框架 

PyTorch（Facebook）动态图

Tensorflow（Google）静态图

caffe预存很多库模型的二进制配置文件 但当网络太大时不便，写脚本生成

使用框架的好处

1. 简单的生成复杂的计算图
2. 自动计算梯度，实现反向传播的细节，自己只需要写出网络的前向传播
3. 忽律CUDA等硬件细节

使用numpy可以进行矩阵计算，但计算梯度时必须自己分别列出算式，且不能在GPU上运行

框架就是让代码和用numpy写差不多，但能自动计算梯度并用GPU运行

eg.

![用tf计算梯度的代码](/Users/jiangtianmeng/Desktop/pics/cnn/用tf计算梯度的代码.png)

tf，静态计算图，先构建一次计算图，然后运行多次

![tf神经网络](/Users/jiangtianmeng/Desktop/pics/cnn/tf神经网络.png)

图中的入口节点，为计算图建立输入槽：x input，w1 w2 weights， y target，损失计算等算式定义：前向传播

用numpy填充数据，然后调用tf.Session反向传播的计算梯度

![tf训练神经网络](/Users/jiangtianmeng/Desktop/pics/cnn/tf训练神经网络.png)

然后训练神经网络

解决用np array更新权重使用cpu和gpu切换的问题，解决tf没有更新的问题

![tf-optimizer](/Users/jiangtianmeng/Desktop/pics/cnn/tf-optimizer.png)

tf还有很多神奇的库，比如layers（带有Xavier初始化自动设定权重和偏差）、loss、keras（封装好的api）、fold（封装好api去实现动态图）

用tensorboard画出损失曲线、可视化计算图

### 9 CNN Architectures

#### AlexNet 

![AlexNet网络结构](/Users/jiangtianmeng/Desktop/pics/cnn/AlexNet网络结构.png)

![AlexNet参数](/Users/jiangtianmeng/Desktop/pics/cnn/AlexNet参数.png)

#### VGG 16

![VGG](/Users/jiangtianmeng/Desktop/pics/cnn/VGG.png)

更小的卷积核 3 * 3

#### GoogleNet

22层，精心设计一个网络架构进行高效计算，没有全连接层，500万个参数，并具有辅助分类输出（softmax函数）将额外的梯度注入下层网络，高效运行

方法是：通过直觉和google计算能力支撑的交叉验证调整

inception module：局部网络拓扑，对进入相同层的相同输入并行计算，将输出串联，保留了尺寸（0padding）并让层次变深。

inception module with dimension reduction：

通过64个1 * 1卷积核作为瓶颈层，降低输出的深度，降低计算量8亿->3亿

![GoogleNet-inception module](/Users/jiangtianmeng/Desktop/pics/cnn/GoogleNet-inception module.png)

![inception-module的具体参数](/Users/jiangtianmeng/Desktop/pics/cnn/inception-module的具体参数.png)

#### ResNet

残差网络，152层，会使用2倍数量的卷积核，没有全连接层，把残差块相互堆叠，也使用瓶颈层降低深度（降维）

好的原因：层次很深？残差？

让每层都尝试学习一些所需函数的底层映射，使用这些块尝试并拟合残差映射

训练的目标不再是block的输出h(x)而是block的输出与输入x之间的残差F(x)

🔍improved：

* 调整ResNet 路径中的层，模块性能更好

* 更宽的残差模块：每个转换层更多卷积核

* resnext：增加宽度：在转换层中建立更多平行的分枝

* 随机的drop
* 密集连接卷积网络
* 增加skip层，改善梯度传递

### 10 Recurrent Neural Networks循环神经网络

#### 10.1 RNN GRU

每个RNN都有循环核心单元，把x作为输入，传入RNN，RNN有一些内部的隐藏状态，隐藏态会在RNN每次读取新的输入时更新，然后下一次输入时隐藏态会将结果反馈至模型。

$h_t =f_w(h_{t-1},x_t)$

新的状态=一些带有参数W的函数（旧的状态，输入向量），每一步时fw不变（权重矩阵不变），得到每一步下计算的梯度最终的w梯度是在所有时步下独立计算出的梯度之和

为了训练模型计算损失函数在w上的梯度，最终的损失值会回溯到每一步的损失，每一步的损失会各自计算出在权重矩阵w上的梯度，他们的总和就是权重w的最终梯度

seq2seq用于机器翻译问题 

包括两个过程encoder&decoder，encoder多对一将输入变成单独的向量，decoder输出序列

#### 10.2 语言模型 

用一个思维向量0001标注一个字母

截断式反向传播

训练模型预测接下去可能出现的词句

#### 10.3 图像标注 视觉问答… 

图像标注：输入一个图像后输出自然语言的图像语义信息

CNN提取图像特征信息后传给RNN生成语句seq

没有学习：LSTM长短记忆网络 long short term memory i输入门f遗忘们g门门o输出门

没有学习：GRU：门控循环单元

### 11 图像目标检测和图像分割 

#### 11.1 图像分割

思想：滑动窗口 不用

思想：全连接卷积网络，全是卷积层，padding保持原有尺寸

对每个像素分配损失，使用交叉熵损失函数计算这个像素的分类与真值像素之间的交叉熵损失，计算总和或平均

应用降采样（最大池化/带步长卷积）和过采样（去池化：重复元素，钉床函数，转置卷积，帮助处理边缘细节）

**可学习的上采样（去卷积层/重卷积/反向卷积）：卷积转置方法**

输入2 * 2，使用转置卷积核，输出4 * 4，用输入做卷积核的权重，输出是带有权重的卷积核的叠加——复制这些有权重的卷积核的值，步长为2滑动填入输出矩阵

![反卷积](/Users/jiangtianmeng/Desktop/pics/cnn/反卷积.png)

#### 11.2 分类和定位（找到边界|姿态定位） 

多重任务损失，两个标量同时最小化，对两种损失加权求和，这个权重是超参数（老师的经验：用你关心的性能组成的矩阵来确定超参数，交叉验证，而不是原本的损失值）

softmax损失衡量分类，l2损失衡量预测的边界坐标和实际坐标（连续值，回归问题用的损失）

#### 11.3 识别/目标检测

**R-CNN**：先长方形框出兴趣区域，设定阈值参数，背景作为一种分类，监督学习，图片和有标签的图片

**fast R-CNN**

用一些搜索算法确定备选区域，但不再根据备选区域直接截取，而是基于备选区域投影到卷积特征映射，然后从卷积特征映射提取属于备选区域的卷积块，因为很多卷积计算可重用所以非常快

因为后续全连接层的输入尺寸固定，所以要用兴趣区域池化层统一卷积块尺寸

在卷层中输入整个图像获取feature map，然后使用一个分离备选区域网络，工作于卷积的特征的上层，在网络内部预测自己的备选区域，然后从备选区域选出块作为输入传入下一层

**YOLO/SSD** 用真实类标去匹配潜在的预测框

### 12 可视化和理解

可视化 为了：看看CNN这个黑盒子到底做了些啥

可视化最后一层全连接层，4096维的向量空间

降维，t-sne降维成2维向量空间，取图像原始的像素放在降维版本的二维坐标上

occlusion实验：弄清楚是图像的那一部分影响分类，如果遮挡的部分影响loss函数急剧变化，就认为这部分图块影响决策，然后绘制热力图。

*这个实验应该可以用来论证，为什么一些对损失函数的操作会有用，也可以用于炼丹

可视化CNN特征：梯度上升算法

一些正则项，使图片看上去更像自然生成的

高斯模糊处理，使图片更平滑

deepdream：放大检测到的图像特征，最大化层特征的范数，重构图像

纹理合成网络：听不懂 格拉姆矩阵？反向传播 L2范数

风格迁移算法：内容图像->风格图像 计算格拉姆矩阵和特征，使用随机噪声初始化输出图像，计算l2范数损失以及图像上的像素梯度，不断重复这些过程，在生成图像上执行梯度上升

### 13 无监督学习--生成式模型

#### 13.1 Pixel RNN、CNN

对一个密度分布显式地建模

pixelRNN从角落开始，生成邻接的图像的像素

pixelCNN每一个像素位置都是一个神经网络的输出（使用softmax函数和分类标签：输入数据的像素值），生成训练集的最大似然

会收到选定的第一个像素的影响

#### 13.2 变分自解码器VAE

在原来的自编码器autoencoder上加入随机成分=>允许生成数据

为了训练模型定义了一个模型函数=>推导并优化下界，下界是变化的，变分即用近似来解函数表达式

算法思想：使用encoder（CNN）获取输入数据x的特征z，然后使用decoder（upconv）获得重构的输入数据

通过最大化数据的参数函数来找到神经网络的参数

损失函数：给定z，对训练像素值取对数，最大化被重构的原始输入数据的似然

训练过程：对每个输入数据的minibatch，前向传播计算各个值，然后反向传播计算各个梯度，不断更新网络参数$\theta$

缺陷：生成图像模糊

#### 13.3 GAN

将随机噪声输入生成器，生成器输出生成假的图像，最小化目标函数

判别器输入假图和真图，最大化目标函数

训练方法，交替固定，反向传播确定另一个网络的参数。

目标函数
$$
\min\limits_{\theta_g}\max\limits_{\theta_d}[\mathbb{E}_{x～~p_{data}}logD_{\theta_d}(x)+\mathbb{E}_{z～p(z)}log(1-D_{\theta_d}(G_{\theta_d}(z)))]
$$
1首先对判别器进行梯度上升，学习最大化目标函数的$\theta_D$
$$
\max\limits_{\theta_d}[\mathbb{E_{x～p_{data}}}logD_{\theta_d}(x)+\mathbb{E}_{z～p(z)}log(1-D_{\theta_d}(G_{\theta_d}(z)))]
$$
2然后对生成器进行梯度下降，学习最小化目标函数的$\theta_G$,
$$
\max\limits_{\theta_g}\mathbb{E}_{z～p(z)}log(D_{\theta_d}(G_{\theta_d}(z)))
$$
训练算法：

对每个训练迭代，先训练判别器网络，然后是生成器网络

对判别器网络的k（超参数？）步训练，从噪声先验分布z中采样得到一个minibatch样本（m个），然后从训练数据X中得到minibatch真实样本（m个），将噪声样本传给生成器网络，并输出伪造图像样本。然后将这个获得的minibatch伪造图像样本和真实样本传入判别器，进行一次梯度计算。然后通过上升这个随机梯度更新判别器参数。

对生成器的训练，获得一个小批量minibatch的噪声样本传入生成器，然后通过优化目标函数更新生成器的参数。

确定：训练不稳定，不是通过直接优化一个损失函数，需要平衡两个网络，可能因为一些查询推断而中断

GAN的优化 各种GAN 变化不大 动动loss

DeepConvGAN，为GAN添加CNN

领域迁移，马->斑马

训练GAN的trickshttps://github.com/soumith/ganhacks

### 14 Deep Reinforcement Learning 强化学习

#### 14.1策略梯度

alpha go

行动 和 奖励

马尔可夫过程就是强化学习的数学公式

pi是状态到行为公式，随机策略采取行动，为了最大化获得奖励：找出最佳策略pi

#### 14.2 Q-Learning, policy gradients算法

找到最佳pi的方法 估计bellman等式 直接收敛于局部最小值$J\theta$

*没有时间细看了

### 15 深度学习的算法和硬件：提高整体效率

算法

1. 修建神经网络 移除一些权重 迭代的剪枝后继续训练 大大减少计算量
2. 权重共享 不保留完全的精度 让相近权值共享一个k-mean的权值 和法1同时使用不会降低准确性
3. 量化 量化权重和激活值 用浮点数更新
4. 低秩近似
5. 二分/三分网络
6. winograd卷积

硬件 tpu充分利用8位整型 foofline模型 充分利用稀疏性

训练 并行化训练 利用16位精度浮点数的优势来进行混合精度训练 稀疏正则项

推断 tpu gpu

